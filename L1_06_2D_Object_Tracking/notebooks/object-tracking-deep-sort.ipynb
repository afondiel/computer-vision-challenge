{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2736560,"sourceType":"datasetVersion","datasetId":1668350}],"dockerImageVersionId":30145,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Deep SORT\nAlgorithm for Object Tracking\n\n- [Paper](https://arxiv.org/abs/1703.07402)\n- [Medium Article](https://towardsdatascience.com/people-tracking-using-deep-learning-5c90d43774be#:~:text=Deep%20Sort%20Algorithm,-I%20love%20the&text=We%20track%20based%20on%20not,factor%20into%20the%20tracking%20logic)\n- [Github Repo](https://github.com/nwojke/deep_sort)\n","metadata":{}},{"cell_type":"markdown","source":"# YOLOv3 Network\nTo detect the objects present in a given frame.\n\nBased on: https://machinelearningmastery.com/how-to-perform-object-detection-with-yolov3-in-keras/","metadata":{}},{"cell_type":"markdown","source":"## Algorithm Implementation Pipeline\n1. ","metadata":{}},{"cell_type":"markdown","source":"## Define and Implement Network Architecture","metadata":{}},{"cell_type":"code","source":"# create a YOLOv3 Keras model and save it to file\n# based on https://github.com/experiencor/keras-yolo3\nimport struct\nimport numpy as np\nimport colorsys\nfrom keras.layers import Conv2D\nfrom keras.layers import Input\nfrom keras.layers import BatchNormalization\nfrom keras.layers import LeakyReLU\nfrom keras.layers import ZeroPadding2D\nfrom keras.layers import UpSampling2D\nfrom keras.layers.merge import add, concatenate\nfrom keras.models import Model\n \ndef _conv_block(inp, convs, skip=True):\n    x = inp\n    count = 0\n    for conv in convs:\n        if count == (len(convs) - 2) and skip:\n            skip_connection = x\n        count += 1\n        if conv['stride'] > 1: x = ZeroPadding2D(((1,0),(1,0)))(x) # peculiar padding as darknet prefer left and top\n        x = Conv2D(conv['filter'],\n                   conv['kernel'],\n                   strides=conv['stride'],\n                   padding='valid' if conv['stride'] > 1 else 'same', # peculiar padding as darknet prefer left and top\n                   name='conv_' + str(conv['layer_idx']),\n                   use_bias=False if conv['bnorm'] else True)(x)\n        if conv['bnorm']: x = BatchNormalization(epsilon=0.001, name='bnorm_' + str(conv['layer_idx']))(x)\n        if conv['leaky']: x = LeakyReLU(alpha=0.1, name='leaky_' + str(conv['layer_idx']))(x)\n    return add([skip_connection, x]) if skip else x\n\ndef make_yolov3_model():\n    input_image = Input(shape=(None, None, 3))\n    # Layer  0 => 4\n    x = _conv_block(input_image, [{'filter': 32, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 0},\n                                  {'filter': 64, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 1},\n                                  {'filter': 32, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 2},\n                                  {'filter': 64, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 3}])\n    # Layer  5 => 8\n    x = _conv_block(x, [{'filter': 128, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 5},\n                        {'filter':  64, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 6},\n                        {'filter': 128, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 7}])\n    # Layer  9 => 11\n    x = _conv_block(x, [{'filter':  64, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 9},\n                        {'filter': 128, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 10}])\n    # Layer 12 => 15\n    x = _conv_block(x, [{'filter': 256, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 12},\n                        {'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 13},\n                        {'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 14}])\n    # Layer 16 => 36\n    for i in range(7):\n        x = _conv_block(x, [{'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 16+i*3},\n                            {'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 17+i*3}])\n    skip_36 = x\n    # Layer 37 => 40\n    x = _conv_block(x, [{'filter': 512, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 37},\n                        {'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 38},\n                        {'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 39}])\n    # Layer 41 => 61\n    for i in range(7):\n        x = _conv_block(x, [{'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 41+i*3},\n                            {'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 42+i*3}])\n    skip_61 = x\n    # Layer 62 => 65\n    x = _conv_block(x, [{'filter': 1024, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 62},\n                        {'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 63},\n                        {'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 64}])\n    # Layer 66 => 74\n    for i in range(3):\n        x = _conv_block(x, [{'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 66+i*3},\n                            {'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 67+i*3}])\n    # Layer 75 => 79\n    x = _conv_block(x, [{'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 75},\n                        {'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 76},\n                        {'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 77},\n                        {'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 78},\n                        {'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 79}], skip=False)\n    # Layer 80 => 82\n    yolo_82 = _conv_block(x, [{'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 80},\n                              {'filter':  255, 'kernel': 1, 'stride': 1, 'bnorm': False, 'leaky': False, 'layer_idx': 81}], skip=False)\n    # Layer 83 => 86\n    x = _conv_block(x, [{'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 84}], skip=False)\n    x = UpSampling2D(2)(x)\n    x = concatenate([x, skip_61])\n    # Layer 87 => 91\n    x = _conv_block(x, [{'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 87},\n                        {'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 88},\n                        {'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 89},\n                        {'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 90},\n                        {'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 91}], skip=False)\n    # Layer 92 => 94\n    yolo_94 = _conv_block(x, [{'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 92},\n                              {'filter': 255, 'kernel': 1, 'stride': 1, 'bnorm': False, 'leaky': False, 'layer_idx': 93}], skip=False)\n    # Layer 95 => 98\n    x = _conv_block(x, [{'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True,   'layer_idx': 96}], skip=False)\n    x = UpSampling2D(2)(x)\n    x = concatenate([x, skip_36])\n    # Layer 99 => 106\n    yolo_106 = _conv_block(x, [{'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 99},\n                               {'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 100},\n                               {'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 101},\n                               {'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 102},\n                               {'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 103},\n                               {'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 104},\n                               {'filter': 255, 'kernel': 1, 'stride': 1, 'bnorm': False, 'leaky': False, 'layer_idx': 105}], skip=False)\n    model = Model(input_image, [yolo_82, yolo_94, yolo_106])\n    return model\n\nclass WeightReader:\n    def __init__(self, weight_file):\n        with open(weight_file, 'rb') as w_f:\n            major,\t= struct.unpack('i', w_f.read(4))\n            minor,\t= struct.unpack('i', w_f.read(4))\n            revision, = struct.unpack('i', w_f.read(4))\n            if (major*10 + minor) >= 2 and major < 1000 and minor < 1000:\n                w_f.read(8)\n            else:\n                w_f.read(4)\n            transpose = (major > 1000) or (minor > 1000)\n            binary = w_f.read()\n        self.offset = 0\n        self.all_weights = np.frombuffer(binary, dtype='float32')\n \n    def read_bytes(self, size):\n        self.offset = self.offset + size\n        return self.all_weights[self.offset-size:self.offset]\n\n    def load_weights(self, model):\n        for i in range(106):\n            try:\n                conv_layer = model.get_layer('conv_' + str(i))\n                print(\"loading weights of convolution #\" + str(i))\n                if i not in [81, 93, 105]:\n                    norm_layer = model.get_layer('bnorm_' + str(i))\n                    size = np.prod(norm_layer.get_weights()[0].shape)\n                    beta  = self.read_bytes(size) # bias\n                    gamma = self.read_bytes(size) # scale\n                    mean  = self.read_bytes(size) # mean\n                    var   = self.read_bytes(size) # variance\n                    weights = norm_layer.set_weights([gamma, beta, mean, var])\n                if len(conv_layer.get_weights()) > 1:\n                    bias   = self.read_bytes(np.prod(conv_layer.get_weights()[1].shape))\n                    kernel = self.read_bytes(np.prod(conv_layer.get_weights()[0].shape))\n                    kernel = kernel.reshape(list(reversed(conv_layer.get_weights()[0].shape)))\n                    kernel = kernel.transpose([2,3,1,0])\n                    conv_layer.set_weights([kernel, bias])\n                else:\n                    kernel = self.read_bytes(np.prod(conv_layer.get_weights()[0].shape))\n                    kernel = kernel.reshape(list(reversed(conv_layer.get_weights()[0].shape)))\n                    kernel = kernel.transpose([2,3,1,0])\n                    conv_layer.set_weights([kernel])\n            except ValueError:\n                print(\"no convolution #\" + str(i))\n\n    def reset(self):\n        self.offset = 0","metadata":{"execution":{"iopub.status.busy":"2024-03-22T18:20:29.657070Z","iopub.execute_input":"2024-03-22T18:20:29.657350Z","iopub.status.idle":"2024-03-22T18:20:29.718692Z","shell.execute_reply.started":"2024-03-22T18:20:29.657321Z","shell.execute_reply":"2024-03-22T18:20:29.717787Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"import cv2\ncv2.__version__","metadata":{"execution":{"iopub.status.busy":"2024-03-22T18:20:31.112625Z","iopub.execute_input":"2024-03-22T18:20:31.112928Z","iopub.status.idle":"2024-03-22T18:20:31.118929Z","shell.execute_reply.started":"2024-03-22T18:20:31.112878Z","shell.execute_reply":"2024-03-22T18:20:31.118207Z"},"trusted":true},"execution_count":31,"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"'4.5.4-dev'"},"metadata":{}}]},{"cell_type":"code","source":"from matplotlib import pyplot\nfrom matplotlib.patches import Rectangle\nfrom numpy import expand_dims\nfrom keras.preprocessing.image import load_img, img_to_array\nimport cv2\nimport random\n\nlabel_map = {\n    \"person\": \"blue\",\n    \"bicycle\": \"yellow\", \n    \"car\": \"red\",\n    \"truck\": \"green\",\n    \"motorbike\": \"white\", \n    \"aeroplane\": \"white\", \n    \"bus\": \"white\",\n    \"train\": \"white\", \n    \"boat\": \"white\"\n}\n \nclass BoundBox:\n    def __init__(self, xmin, ymin, xmax, ymax, objness = None, classes = None):\n        self.xmin = xmin\n        self.ymin = ymin\n        self.xmax = xmax\n        self.ymax = ymax\n        self.objness = objness\n        self.classes = classes\n        self.label = -1\n        self.score = -1\n\n    def get_label(self):\n        if self.label == -1:\n            self.label = np.argmax(self.classes)\n\n        return self.label\n\n    def get_score(self):\n        if self.score == -1:\n            self.score = self.classes[self.get_label()]\n \n        return self.score\n \ndef _sigmoid(x):\n    return 1. / (1. + np.exp(-x))\n \ndef decode_netout(netout, anchors, obj_thresh, net_h, net_w):\n    grid_h, grid_w = netout.shape[:2]\n    nb_box = 3\n    netout = netout.reshape((grid_h, grid_w, nb_box, -1))\n    nb_class = netout.shape[-1] - 5\n    boxes = []\n    netout[..., :2]  = _sigmoid(netout[..., :2])\n    netout[..., 4:]  = _sigmoid(netout[..., 4:])\n    netout[..., 5:]  = netout[..., 4][..., np.newaxis] * netout[..., 5:]\n    netout[..., 5:] *= netout[..., 5:] > obj_thresh\n \n    for i in range(grid_h*grid_w):\n        row = i / grid_w\n        col = i % grid_w\n        for b in range(nb_box):\n            # 4th element is objectness score\n            objectness = netout[int(row)][int(col)][b][4]\n            if(objectness.all() <= obj_thresh): continue\n            # first 4 elements are x, y, w, and h\n            x, y, w, h = netout[int(row)][int(col)][b][:4]\n            x = (col + x) / grid_w # center position, unit: image width\n            y = (row + y) / grid_h # center position, unit: image height\n            w = anchors[2 * b + 0] * np.exp(w) / net_w # unit: image width\n            h = anchors[2 * b + 1] * np.exp(h) / net_h # unit: image height\n            # last elements are class probabilities\n            classes = netout[int(row)][col][b][5:]\n            box = BoundBox(x-w/2, y-h/2, x+w/2, y+h/2, objectness, classes)\n            boxes.append(box)\n    return boxes\n \ndef correct_yolo_boxes(boxes, image_h, image_w, net_h, net_w):\n    new_w, new_h = net_w, net_h\n    for i in range(len(boxes)):\n        x_offset, x_scale = (net_w - new_w)/2./net_w, float(new_w)/net_w\n        y_offset, y_scale = (net_h - new_h)/2./net_h, float(new_h)/net_h\n        boxes[i].xmin = int((boxes[i].xmin - x_offset) / x_scale * image_w)\n        boxes[i].xmax = int((boxes[i].xmax - x_offset) / x_scale * image_w)\n        boxes[i].ymin = int((boxes[i].ymin - y_offset) / y_scale * image_h)\n        boxes[i].ymax = int((boxes[i].ymax - y_offset) / y_scale * image_h)\n\ndef _interval_overlap(interval_a, interval_b):\n    x1, x2 = interval_a\n    x3, x4 = interval_b\n    if x3 < x1:\n        if x4 < x1:\n            return 0\n        else:\n            return min(x2,x4) - x1\n    else:\n        if x2 < x3:\n            return 0\n        else:\n            return min(x2,x4) - x3\n\ndef bbox_iou(box1, box2):\n    intersect_w = _interval_overlap([box1.xmin, box1.xmax], [box2.xmin, box2.xmax])\n    intersect_h = _interval_overlap([box1.ymin, box1.ymax], [box2.ymin, box2.ymax])\n    intersect = intersect_w * intersect_h\n    w1, h1 = box1.xmax-box1.xmin, box1.ymax-box1.ymin\n    w2, h2 = box2.xmax-box2.xmin, box2.ymax-box2.ymin\n    union = w1*h1 + w2*h2 - intersect\n    return float(intersect) / union\n \ndef do_nms(boxes, nms_thresh):\n    if len(boxes) > 0:\n        nb_class = len(boxes[0].classes)\n    else:\n        return\n    for c in range(nb_class):\n        sorted_indices = np.argsort([-box.classes[c] for box in boxes])\n        for i in range(len(sorted_indices)):\n            index_i = sorted_indices[i]\n            if boxes[index_i].classes[c] == 0: continue\n            for j in range(i+1, len(sorted_indices)):\n                index_j = sorted_indices[j]\n                if bbox_iou(boxes[index_i], boxes[index_j]) >= nms_thresh:\n                    boxes[index_j].classes[c] = 0\n\n# load and prepare an image\ndef load_image_pixels(filename, shape):\n    # load the image to get its shape\n    image = load_img(filename)\n    width, height = image.size\n    # load the image with the required size\n    image = load_img(filename, target_size=shape)\n    # convert to numpy array\n    image = img_to_array(image)\n    # scale pixel values to [0, 1]\n    image = image.astype('float32')\n    image /= 255.0\n    # add a dimension so that we have one sample\n    image = expand_dims(image, 0)\n    return image, width, height\n\n# Preprocess the given image\ndef image_preprocess(image, target_size):\n    ih, iw    = target_size\n    h,  w, _  = image.shape\n\n    scale = min(iw/w, ih/h)\n    nw, nh  = int(scale * w), int(scale * h)\n    image_resized = cv2.resize(image, (nw, nh))\n\n    image_paded = np.full(shape=[ih, iw, 3], fill_value=128.0)\n    dw, dh = (iw - nw) // 2, (ih-nh) // 2\n    image_paded[dh:nh+dh, dw:nw+dw, :] = image_resized\n    image_paded = image_paded / 255.\n\n    image_expanded = expand_dims(image_paded, 0)\n\n    return image_expanded, w, h\n \n# get all of the results above a threshold\ndef get_boxes(boxes, labels, thresh):\n    v_boxes, v_labels, v_scores = list(), list(), list()\n    # enumerate all boxes\n    for box in boxes:\n        # enumerate all possible labels\n        for i in range(len(labels)):\n            # check if the threshold for this label is high enough\n            if box.classes[i] > thresh:\n                v_boxes.append(box)\n                v_labels.append(labels[i])\n                v_scores.append(box.classes[i]*100)\n                # don't break, many labels may trigger for one box\n    return v_boxes, v_labels, v_scores\n \n# draw all results\ndef draw_boxes(filename, v_boxes, v_labels, v_scores):\n    # load the image\n    data = pyplot.imread(filename)\n    # plot the image\n    pyplot.imshow(data)\n    # get the context for drawing boxes\n    ax = pyplot.gca()\n    # plot each box\n    for i in range(len(v_boxes)):\n        box = v_boxes[i]\n        # get coordinates\n        y1, x1, y2, x2 = box.ymin, box.xmin, box.ymax, box.xmax\n        # calculate width and height of the box\n        width, height = x2 - x1, y2 - y1\n        # create the shape\n        rect = Rectangle((x1, y1), width, height, fill=False, color=label_map[v_labels[i]])\n        # draw the box\n        ax.add_patch(rect)\n        # draw text and score in top left corner\n        label = \"%s (%.3f)\" % (v_labels[i], v_scores[i])\n        pyplot.text(x1, y1, label, color=label_map[v_labels[i]])\n    # show the plot\n    pyplot.show()\n    \ndef draw_bbox(image, bboxes, CLASSES, show_label=True, show_confidence = True, Text_colors=(255,255,0), rectangle_colors='', tracking=False):   \n    NUM_CLASS = CLASSES\n    num_classes = len(NUM_CLASS)\n    image_h, image_w, _ = image.shape\n    hsv_tuples = [(1.0 * x / num_classes, 1., 1.) for x in range(num_classes)]\n    \n    colors = list(map(lambda x: colorsys.hsv_to_rgb(*x), hsv_tuples))\n    colors = list(map(lambda x: (int(x[0] * 255), int(x[1] * 255), int(x[2] * 255)), colors))\n\n    random.seed(0)\n    random.shuffle(colors)\n    random.seed(None)\n\n    for i, bbox in enumerate(bboxes):\n        coor = np.array(bbox[:4], dtype=np.int32)\n        score = bbox[4]\n        class_ind = int(bbox[5])\n        \n        bbox_color = rectangle_colors if rectangle_colors != '' else colors[class_ind]\n        bbox_thick = int(0.6 * (image_h + image_w) / 1000)\n        \n        if bbox_thick < 1: bbox_thick = 1\n        fontScale = 0.75 * bbox_thick\n        (x1, y1), (x2, y2) = (coor[0], coor[1]), (coor[2], coor[3])\n\n        # put object rectangle\n        cv2.rectangle(image, (x1, y1), (x2, y2), bbox_color, bbox_thick*2)\n\n        if show_label:\n            # get text label\n            score_str = \" {:.2f}\".format(score) if show_confidence else \"\"\n\n            if tracking: score_str = \" \"+str(score)\n\n            label = \"{}\".format(NUM_CLASS[class_ind]) + score_str\n\n            # get text size\n            (text_width, text_height), baseline = cv2.getTextSize(label, cv2.FONT_HERSHEY_COMPLEX_SMALL,\n                                                                  fontScale, thickness=bbox_thick)\n            # put filled text rectangle\n            cv2.rectangle(image, (x1, y1), (x1 + text_width, y1 - text_height - baseline), bbox_color, thickness=cv2.FILLED)\n\n            # put text above rectangle\n            cv2.putText(image, label, (x1, y1-4), cv2.FONT_HERSHEY_COMPLEX_SMALL,\n                        fontScale, Text_colors, bbox_thick, lineType=cv2.LINE_AA)\n\n    return image","metadata":{"execution":{"iopub.status.busy":"2024-03-22T18:20:32.016157Z","iopub.execute_input":"2024-03-22T18:20:32.016487Z","iopub.status.idle":"2024-03-22T18:20:32.072494Z","shell.execute_reply.started":"2024-03-22T18:20:32.016450Z","shell.execute_reply":"2024-03-22T18:20:32.071644Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"NUM_CLASS = {\n    0: 'person',\n    1: 'bicycle',\n    2: 'car',\n    3: 'motorbike',\n    4: 'aeroplane',\n    5: 'bus',\n    6: 'train',\n    7: 'truck',\n    8: 'boat',\n    9: 'traffic-light',\n    10: 'fire-hydrant',\n    11: 'stop-sign',\n    12: 'parking-meter',\n    13: 'bench',\n    14: 'bird',\n    15: 'cat',\n    16: 'dog',\n    17: 'horse',\n    18: 'sheep',\n    19: 'cow',\n    20: 'elephant',\n    21: 'bear',\n    22: 'zebra',\n    23: 'giraffe',\n    24: 'backpack',\n    25: 'umbrella',\n    26: 'handbag',\n    27: 'tie',\n    28: 'suitcase',\n    29: 'frisbee',\n    30: 'skis',\n    31: 'snowboard',\n    32: 'sports-ball',\n    33: 'kite',\n    34: 'baseball-bat',\n    35: 'baseball-glove',\n    36: 'skateboard',\n    37: 'surfboard',\n    38: 'tennis-racket',\n    39: 'bottle',\n    40: 'wine-glass',\n    41: 'cup',\n    42: 'fork',\n    43: 'knife',\n    44: 'spoon',\n    45: 'bowl',\n    46: 'banana',\n    47: 'apple',\n    48: 'sandwich',\n    49: 'orange',\n    50: 'broccoli',\n    51: 'carrot',\n    52: 'hot-dog',\n    53: 'pizza',\n    54: 'donut',\n    55: 'cake',\n    56: 'chair',\n    57: 'sofa',\n    58: 'pottedplant',\n    59: 'bed',\n    60: 'diningtable',\n    61: 'toilet',\n    62: 'tvmonitor',\n    63: 'laptop',\n    64: 'mouse',\n    65: 'remote',\n    66: 'keyboard',\n    67: 'cell-phone',\n    68: 'microwave',\n    69: 'oven',\n    70: 'toaster',\n    71: 'sink',\n    72: 'refrigerator',\n    73: 'book',\n    74: 'clock',\n    75: 'vase',\n    76: 'scissors',\n    77: 'teddy-bear', \n    78: 'hair-drier',\n    79: 'toothbrush'\n}","metadata":{"execution":{"iopub.status.busy":"2024-03-22T18:20:32.851298Z","iopub.execute_input":"2024-03-22T18:20:32.851596Z","iopub.status.idle":"2024-03-22T18:20:32.864149Z","shell.execute_reply.started":"2024-03-22T18:20:32.851565Z","shell.execute_reply":"2024-03-22T18:20:32.863290Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"## Download and Load Weights","metadata":{}},{"cell_type":"code","source":"!wget https://pjreddie.com/media/files/yolov3.weights -O yolov3.weights","metadata":{"execution":{"iopub.status.busy":"2024-03-22T18:20:33.751985Z","iopub.execute_input":"2024-03-22T18:20:33.752887Z","iopub.status.idle":"2024-03-22T18:21:20.324054Z","shell.execute_reply.started":"2024-03-22T18:20:33.752842Z","shell.execute_reply":"2024-03-22T18:21:20.323253Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"--2024-03-22 18:20:34--  https://pjreddie.com/media/files/yolov3.weights\nResolving pjreddie.com (pjreddie.com)... 162.0.215.52\nConnecting to pjreddie.com (pjreddie.com)|162.0.215.52|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 248007048 (237M) [application/octet-stream]\nSaving to: ‘yolov3.weights’\n\nyolov3.weights      100%[===================>] 236.52M  12.5MB/s    in 45s     \n\n2024-03-22 18:21:20 (5.30 MB/s) - ‘yolov3.weights’ saved [248007048/248007048]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# define the model\nmodel = make_yolov3_model()\n\n# load the model weights\n# I have loaded the pretrained weights in a separate dataset\nweight_reader = WeightReader('yolov3.weights')\n\n# set the model weights into the model\nweight_reader.load_weights(model)\n\n# save the model to file\nmodel.save('model.h5')","metadata":{"execution":{"iopub.status.busy":"2024-03-22T18:21:20.325954Z","iopub.execute_input":"2024-03-22T18:21:20.326210Z","iopub.status.idle":"2024-03-22T18:21:24.064786Z","shell.execute_reply.started":"2024-03-22T18:21:20.326178Z","shell.execute_reply":"2024-03-22T18:21:24.063729Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"loading weights of convolution #0\nloading weights of convolution #1\nloading weights of convolution #2\nloading weights of convolution #3\nno convolution #4\nloading weights of convolution #5\nloading weights of convolution #6\nloading weights of convolution #7\nno convolution #8\nloading weights of convolution #9\nloading weights of convolution #10\nno convolution #11\nloading weights of convolution #12\nloading weights of convolution #13\nloading weights of convolution #14\nno convolution #15\nloading weights of convolution #16\nloading weights of convolution #17\nno convolution #18\nloading weights of convolution #19\nloading weights of convolution #20\nno convolution #21\nloading weights of convolution #22\nloading weights of convolution #23\nno convolution #24\nloading weights of convolution #25\nloading weights of convolution #26\nno convolution #27\nloading weights of convolution #28\nloading weights of convolution #29\nno convolution #30\nloading weights of convolution #31\nloading weights of convolution #32\nno convolution #33\nloading weights of convolution #34\nloading weights of convolution #35\nno convolution #36\nloading weights of convolution #37\nloading weights of convolution #38\nloading weights of convolution #39\nno convolution #40\nloading weights of convolution #41\nloading weights of convolution #42\nno convolution #43\nloading weights of convolution #44\nloading weights of convolution #45\nno convolution #46\nloading weights of convolution #47\nloading weights of convolution #48\nno convolution #49\nloading weights of convolution #50\nloading weights of convolution #51\nno convolution #52\nloading weights of convolution #53\nloading weights of convolution #54\nno convolution #55\nloading weights of convolution #56\nloading weights of convolution #57\nno convolution #58\nloading weights of convolution #59\nloading weights of convolution #60\nno convolution #61\nloading weights of convolution #62\nloading weights of convolution #63\nloading weights of convolution #64\nno convolution #65\nloading weights of convolution #66\nloading weights of convolution #67\nno convolution #68\nloading weights of convolution #69\nloading weights of convolution #70\nno convolution #71\nloading weights of convolution #72\nloading weights of convolution #73\nno convolution #74\nloading weights of convolution #75\nloading weights of convolution #76\nloading weights of convolution #77\nloading weights of convolution #78\nloading weights of convolution #79\nloading weights of convolution #80\nloading weights of convolution #81\nno convolution #82\nno convolution #83\nloading weights of convolution #84\nno convolution #85\nno convolution #86\nloading weights of convolution #87\nloading weights of convolution #88\nloading weights of convolution #89\nloading weights of convolution #90\nloading weights of convolution #91\nloading weights of convolution #92\nloading weights of convolution #93\nno convolution #94\nno convolution #95\nloading weights of convolution #96\nno convolution #97\nno convolution #98\nloading weights of convolution #99\nloading weights of convolution #100\nloading weights of convolution #101\nloading weights of convolution #102\nloading weights of convolution #103\nloading weights of convolution #104\nloading weights of convolution #105\n","output_type":"stream"}]},{"cell_type":"code","source":"# load yolov3 model\nfrom keras.models import load_model\nmodel = load_model('model.h5')","metadata":{"execution":{"iopub.status.busy":"2024-03-22T18:21:24.067100Z","iopub.execute_input":"2024-03-22T18:21:24.067555Z","iopub.status.idle":"2024-03-22T18:21:25.923150Z","shell.execute_reply.started":"2024-03-22T18:21:24.067502Z","shell.execute_reply":"2024-03-22T18:21:25.922317Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"# Parameters used in the Dataset, on which YOLOv3 was pretrained\nanchors = [[116,90, 156,198, 373,326], [30,61, 62,45, 59,119], [10,13, 16,30, 33,23]]\n\n# define the expected input shape for the model\nWIDTH, HEIGHT = 416, 416\n\n# define the probability threshold for detected objects\nclass_threshold = 0.8","metadata":{"execution":{"iopub.status.busy":"2024-03-22T18:21:25.925246Z","iopub.execute_input":"2024-03-22T18:21:25.925547Z","iopub.status.idle":"2024-03-22T18:21:25.933153Z","shell.execute_reply.started":"2024-03-22T18:21:25.925516Z","shell.execute_reply":"2024-03-22T18:21:25.932330Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"# Detections\n\nTo save and encode the detections in a given frame.","metadata":{}},{"cell_type":"code","source":"import os\nimport errno\nimport argparse\nimport numpy as np\nimport cv2\nimport tensorflow.compat.v1 as tf\nimport scipy.linalg\nfrom scipy.optimize import linear_sum_assignment","metadata":{"execution":{"iopub.status.busy":"2024-03-22T18:21:25.934408Z","iopub.execute_input":"2024-03-22T18:21:25.934640Z","iopub.status.idle":"2024-03-22T18:21:26.803801Z","shell.execute_reply.started":"2024-03-22T18:21:25.934610Z","shell.execute_reply":"2024-03-22T18:21:26.802869Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"class Detection(object):\n    \"\"\"\n    This class represents a bounding box detection in a single image.\n    Parameters\n    ----------\n    tlwh : array_like\n        Bounding box in format `(x, y, w, h)`.\n    confidence : float\n        Detector confidence score.\n    feature : array_like\n        A feature vector that describes the object contained in this image.\n    Attributes\n    ----------\n    tlwh : ndarray\n        Bounding box in format `(top left x, top left y, width, height)`.\n    confidence : ndarray\n        Detector confidence score.\n    class_name : ndarray\n        Detector class.\n    feature : ndarray | NoneType\n        A feature vector that describes the object contained in this image.\n    \"\"\"\n\n    def __init__(self, tlwh, confidence, class_name, feature):\n        self.tlwh = np.asarray(tlwh, dtype=np.float)\n        self.confidence = float(confidence)\n        self.class_name = class_name\n        self.feature = np.asarray(feature, dtype=np.float32)\n\n    def get_class(self):\n        return self.class_name\n\n    def to_tlbr(self):\n        \"\"\"Convert bounding box to format `(min x, min y, max x, max y)`, i.e.,\n        `(top left, bottom right)`.\n        \"\"\"\n        ret = self.tlwh.copy()\n        ret[2:] += ret[:2]\n        return ret\n\n    def to_xyah(self):\n        \"\"\"Convert bounding box to format `(center x, center y, aspect ratio,\n        height)`, where the aspect ratio is `width / height`.\n        \"\"\"\n        ret = self.tlwh.copy()\n        ret[:2] += ret[2:] / 2\n        ret[2] /= ret[3]\n        return ret\n\nclass ImageEncoder(object):\n\n    def __init__(self, checkpoint_filename, input_name=\"images\", output_name=\"features\"):\n        self.session = tf.Session()\n        with tf.gfile.GFile(checkpoint_filename, \"rb\") as file_handle:\n            graph_def = tf.GraphDef()\n            graph_def.ParseFromString(file_handle.read())\n        tf.import_graph_def(graph_def)\n        try:\n            self.input_var = tf.get_default_graph().get_tensor_by_name(input_name)\n            self.output_var = tf.get_default_graph().get_tensor_by_name(output_name)\n        except KeyError:\n            layers = [i.name for i in tf.get_default_graph().get_operations()]\n            self.input_var = tf.get_default_graph().get_tensor_by_name(layers[0]+':0')\n            self.output_var = tf.get_default_graph().get_tensor_by_name(layers[-1]+':0')            \n\n        assert len(self.output_var.get_shape()) == 2\n        assert len(self.input_var.get_shape()) == 4\n        self.feature_dim = self.output_var.get_shape().as_list()[-1]\n        self.image_shape = self.input_var.get_shape().as_list()[1:]\n\n    def __call__(self, data_x, batch_size=32):\n        out = np.zeros((len(data_x), self.feature_dim), np.float32)\n        _run_in_batches(\n            lambda x: self.session.run(self.output_var, feed_dict=x),\n            {self.input_var: data_x}, out, batch_size)\n        return out","metadata":{"execution":{"iopub.status.busy":"2024-03-22T18:21:26.805022Z","iopub.execute_input":"2024-03-22T18:21:26.805296Z","iopub.status.idle":"2024-03-22T18:21:26.823194Z","shell.execute_reply.started":"2024-03-22T18:21:26.805237Z","shell.execute_reply":"2024-03-22T18:21:26.822413Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"def _run_in_batches(f, data_dict, out, batch_size):\n    data_len = len(out)\n    num_batches = int(data_len / batch_size)\n\n    s, e = 0, 0\n    for i in range(num_batches):\n        s, e = i * batch_size, (i + 1) * batch_size\n        batch_data_dict = {k: v[s:e] for k, v in data_dict.items()}\n        out[s:e] = f(batch_data_dict)\n    if e < len(out):\n        batch_data_dict = {k: v[e:] for k, v in data_dict.items()}\n        out[e:] = f(batch_data_dict)\n\n\ndef extract_image_patch(image, bbox, patch_shape):\n    \"\"\"Extract image patch from bounding box.\n    Parameters\n    ----------\n    image : ndarray\n        The full image.\n    bbox : array_like\n        The bounding box in format (x, y, width, height).\n    patch_shape : Optional[array_like]\n        This parameter can be used to enforce a desired patch shape\n        (height, width). First, the `bbox` is adapted to the aspect ratio\n        of the patch shape, then it is clipped at the image boundaries.\n        If None, the shape is computed from :arg:`bbox`.\n    Returns\n    -------\n    ndarray | NoneType\n        An image patch showing the :arg:`bbox`, optionally reshaped to\n        :arg:`patch_shape`.\n        Returns None if the bounding box is empty or fully outside of the image\n        boundaries.\n    \"\"\"\n    bbox = np.array(bbox)\n    if patch_shape is not None:\n        # correct aspect ratio to patch shape\n        target_aspect = float(patch_shape[1]) / patch_shape[0]\n        new_width = target_aspect * bbox[3]\n        bbox[0] -= (new_width - bbox[2]) / 2\n        bbox[2] = new_width\n\n    # convert to top left, bottom right\n    bbox[2:] += bbox[:2]\n    bbox = bbox.astype(np.int)\n\n    # clip at image boundaries\n    bbox[:2] = np.maximum(0, bbox[:2])\n    bbox[2:] = np.minimum(np.asarray(image.shape[:2][::-1]) - 1, bbox[2:])\n    if np.any(bbox[:2] >= bbox[2:]):\n        return None\n    sx, sy, ex, ey = bbox\n    image = image[sy:ey, sx:ex]\n    image = cv2.resize(image, tuple(patch_shape[::-1]))\n    return image\n\ndef create_box_encoder(model_filename, input_name=\"images:0\", output_name=\"features:0\", batch_size=32):\n    image_encoder = ImageEncoder(model_filename, input_name, output_name)\n    image_shape = image_encoder.image_shape\n\n    def encoder(image, boxes):\n        image_patches = []\n        for box in boxes:\n            patch = extract_image_patch(image, box, image_shape[:2])\n            if patch is None:\n                print(\"WARNING: Failed to extract image patch: %s.\" % str(box))\n                patch = np.random.uniform(0., 255., image_shape).astype(np.uint8)\n            image_patches.append(patch)\n        image_patches = np.asarray(image_patches)\n        return image_encoder(image_patches, batch_size)\n\n    return encoder\n\n\ndef generate_detections(encoder, mot_dir, output_dir, detection_dir=None):\n    \"\"\"Generate detections with features.\n    Parameters\n    ----------\n    encoder : Callable[image, ndarray] -> ndarray\n        The encoder function takes as input a BGR color image and a matrix of\n        bounding boxes in format `(x, y, w, h)` and returns a matrix of\n        corresponding feature vectors.\n    mot_dir : str\n        Path to the MOTChallenge directory (can be either train or test).\n    output_dir\n        Path to the output directory. Will be created if it does not exist.\n    detection_dir\n        Path to custom detections. The directory structure should be the default\n        MOTChallenge structure: `[sequence]/det/det.txt`. If None, uses the\n        standard MOTChallenge detections.\n    \"\"\"\n    if detection_dir is None:\n        detection_dir = mot_dir\n    try:\n        os.makedirs(output_dir)\n    except OSError as exception:\n        if exception.errno == errno.EEXIST and os.path.isdir(output_dir):\n            pass\n        else:\n            raise ValueError(\n                \"Failed to created output directory '%s'\" % output_dir)\n\n    for sequence in os.listdir(mot_dir):\n        print(\"Processing %s\" % sequence)\n        sequence_dir = os.path.join(mot_dir, sequence)\n\n        image_dir = os.path.join(sequence_dir, \"img1\")\n        image_filenames = {\n            int(os.path.splitext(f)[0]): os.path.join(image_dir, f)\n            for f in os.listdir(image_dir)}\n\n        detection_file = os.path.join(\n            detection_dir, sequence, \"det/det.txt\")\n        detections_in = np.loadtxt(detection_file, delimiter=',')\n        detections_out = []\n\n        frame_indices = detections_in[:, 0].astype(np.int)\n        min_frame_idx = frame_indices.astype(np.int).min()\n        max_frame_idx = frame_indices.astype(np.int).max()\n        for frame_idx in range(min_frame_idx, max_frame_idx + 1):\n            print(\"Frame %05d/%05d\" % (frame_idx, max_frame_idx))\n            mask = frame_indices == frame_idx\n            rows = detections_in[mask]\n\n            if frame_idx not in image_filenames:\n                print(\"WARNING could not find image for frame %d\" % frame_idx)\n                continue\n            bgr_image = cv2.imread(\n                image_filenames[frame_idx], cv2.IMREAD_COLOR)\n            features = encoder(bgr_image, rows[:, 2:6].copy())\n            detections_out += [np.r_[(row, feature)] for row, feature\n                               in zip(rows, features)]\n\n        output_filename = os.path.join(output_dir, \"%s.npy\" % sequence)\n        np.save(\n            output_filename, np.asarray(detections_out), allow_pickle=False)","metadata":{"execution":{"iopub.status.busy":"2024-03-22T18:21:26.824483Z","iopub.execute_input":"2024-03-22T18:21:26.824710Z","iopub.status.idle":"2024-03-22T18:21:26.852921Z","shell.execute_reply.started":"2024-03-22T18:21:26.824682Z","shell.execute_reply":"2024-03-22T18:21:26.852315Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"!wget https://github.com/anushkadhiman/ObjectTracking-DeepSORT-YOLOv3-TF2/raw/master/model_data/coco/mars-small128.pb","metadata":{"execution":{"iopub.status.busy":"2024-03-22T18:21:26.853876Z","iopub.execute_input":"2024-03-22T18:21:26.854098Z","iopub.status.idle":"2024-03-22T18:21:28.800914Z","shell.execute_reply.started":"2024-03-22T18:21:26.854070Z","shell.execute_reply":"2024-03-22T18:21:28.799775Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"--2024-03-22 18:21:27--  https://github.com/anushkadhiman/ObjectTracking-DeepSORT-YOLOv3-TF2/raw/master/model_data/coco/mars-small128.pb\nResolving github.com (github.com)... 140.82.121.4\nConnecting to github.com (github.com)|140.82.121.4|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://raw.githubusercontent.com/anushkadhiman/ObjectTracking-DeepSORT-YOLOv3-TF2/master/model_data/coco/mars-small128.pb [following]\n--2024-03-22 18:21:27--  https://raw.githubusercontent.com/anushkadhiman/ObjectTracking-DeepSORT-YOLOv3-TF2/master/model_data/coco/mars-small128.pb\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 11244842 (11M) [application/octet-stream]\nSaving to: ‘mars-small128.pb.2’\n\nmars-small128.pb.2  100%[===================>]  10.72M  --.-KB/s    in 0.06s   \n\n2024-03-22 18:21:28 (190 MB/s) - ‘mars-small128.pb.2’ saved [11244842/11244842]\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Kalman Filter\n\nTo predict the motion of the objects in a given frame.","metadata":{}},{"cell_type":"code","source":"\"\"\"\nTable for the 0.95 quantile of the chi-square distribution with N degrees of\nfreedom (contains values for N=1, ..., 9). Taken from MATLAB/Octave's chi2inv\nfunction and used as Mahalanobis gating threshold.\n\"\"\"\nchi2inv95 = {\n    1: 3.8415,\n    2: 5.9915,\n    3: 7.8147,\n    4: 9.4877,\n    5: 11.070,\n    6: 12.592,\n    7: 14.067,\n    8: 15.507,\n    9: 16.919}\n\n\nclass KalmanFilter(object):\n    \"\"\"\n    A simple Kalman filter for tracking bounding boxes in image space.\n    The 8-dimensional state space\n        x, y, a, h, vx, vy, va, vh\n    contains the bounding box center position (x, y), aspect ratio a, height h,\n    and their respective velocities.\n    Object motion follows a constant velocity model. The bounding box location\n    (x, y, a, h) is taken as direct observation of the state space (linear\n    observation model).\n    \"\"\"\n\n    def __init__(self):\n        ndim, dt = 4, 1.\n\n        # Create Kalman filter model matrices.\n        self._motion_mat = np.eye(2 * ndim, 2 * ndim)\n        for i in range(ndim):\n            self._motion_mat[i, ndim + i] = dt\n        self._update_mat = np.eye(ndim, 2 * ndim)\n\n        # Motion and observation uncertainty are chosen relative to the current\n        # state estimate. These weights control the amount of uncertainty in\n        # the model. This is a bit hacky.\n        self._std_weight_position = 1. / 20\n        self._std_weight_velocity = 1. / 160\n\n    def initiate(self, measurement):\n        \"\"\"Create track from unassociated measurement.\n        Parameters\n        ----------\n        measurement : ndarray\n            Bounding box coordinates (x, y, a, h) with center position (x, y),\n            aspect ratio a, and height h.\n        Returns\n        -------\n        (ndarray, ndarray)\n            Returns the mean vector (8 dimensional) and covariance matrix (8x8\n            dimensional) of the new track. Unobserved velocities are initialized\n            to 0 mean.\n        \"\"\"\n        mean_pos = measurement\n        mean_vel = np.zeros_like(mean_pos)\n        mean = np.r_[mean_pos, mean_vel]\n\n        std = [\n            2 * self._std_weight_position * measurement[3],\n            2 * self._std_weight_position * measurement[3],\n            1e-2,\n            2 * self._std_weight_position * measurement[3],\n            10 * self._std_weight_velocity * measurement[3],\n            10 * self._std_weight_velocity * measurement[3],\n            1e-5,\n            10 * self._std_weight_velocity * measurement[3]]\n        covariance = np.diag(np.square(std))\n        return mean, covariance\n\n    def predict(self, mean, covariance):\n        \"\"\"Run Kalman filter prediction step.\n        Parameters\n        ----------\n        mean : ndarray\n            The 8 dimensional mean vector of the object state at the previous\n            time step.\n        covariance : ndarray\n            The 8x8 dimensional covariance matrix of the object state at the\n            previous time step.\n        Returns\n        -------\n        (ndarray, ndarray)\n            Returns the mean vector and covariance matrix of the predicted\n            state. Unobserved velocities are initialized to 0 mean.\n        \"\"\"\n        std_pos = [\n            self._std_weight_position * mean[3],\n            self._std_weight_position * mean[3],\n            1e-2,\n            self._std_weight_position * mean[3]]\n        std_vel = [\n            self._std_weight_velocity * mean[3],\n            self._std_weight_velocity * mean[3],\n            1e-5,\n            self._std_weight_velocity * mean[3]]\n        motion_cov = np.diag(np.square(np.r_[std_pos, std_vel]))\n\n        mean = np.dot(self._motion_mat, mean)\n        covariance = np.linalg.multi_dot((\n            self._motion_mat, covariance, self._motion_mat.T)) + motion_cov\n\n        return mean, covariance\n\n    def project(self, mean, covariance):\n        \"\"\"Project state distribution to measurement space.\n        Parameters\n        ----------\n        mean : ndarray\n            The state's mean vector (8 dimensional array).\n        covariance : ndarray\n            The state's covariance matrix (8x8 dimensional).\n        Returns\n        -------\n        (ndarray, ndarray)\n            Returns the projected mean and covariance matrix of the given state\n            estimate.\n        \"\"\"\n        std = [\n            self._std_weight_position * mean[3],\n            self._std_weight_position * mean[3],\n            1e-1,\n            self._std_weight_position * mean[3]]\n        innovation_cov = np.diag(np.square(std))\n\n        mean = np.dot(self._update_mat, mean)\n        covariance = np.linalg.multi_dot((\n            self._update_mat, covariance, self._update_mat.T))\n        return mean, covariance + innovation_cov\n\n    def update(self, mean, covariance, measurement):\n        \"\"\"Run Kalman filter correction step.\n        Parameters\n        ----------\n        mean : ndarray\n            The predicted state's mean vector (8 dimensional).\n        covariance : ndarray\n            The state's covariance matrix (8x8 dimensional).\n        measurement : ndarray\n            The 4 dimensional measurement vector (x, y, a, h), where (x, y)\n            is the center position, a the aspect ratio, and h the height of the\n            bounding box.\n        Returns\n        -------\n        (ndarray, ndarray)\n            Returns the measurement-corrected state distribution.\n        \"\"\"\n        projected_mean, projected_cov = self.project(mean, covariance)\n\n        chol_factor, lower = scipy.linalg.cho_factor(\n            projected_cov, lower=True, check_finite=False)\n        kalman_gain = scipy.linalg.cho_solve(\n            (chol_factor, lower), np.dot(covariance, self._update_mat.T).T,\n            check_finite=False).T\n        innovation = measurement - projected_mean\n\n        new_mean = mean + np.dot(innovation, kalman_gain.T)\n        new_covariance = covariance - np.linalg.multi_dot((\n            kalman_gain, projected_cov, kalman_gain.T))\n        return new_mean, new_covariance\n\n    def gating_distance(self, mean, covariance, measurements,\n                        only_position=False):\n        \"\"\"Compute gating distance between state distribution and measurements.\n        A suitable distance threshold can be obtained from `chi2inv95`. If\n        `only_position` is False, the chi-square distribution has 4 degrees of\n        freedom, otherwise 2.\n        Parameters\n        ----------\n        mean : ndarray\n            Mean vector over the state distribution (8 dimensional).\n        covariance : ndarray\n            Covariance of the state distribution (8x8 dimensional).\n        measurements : ndarray\n            An Nx4 dimensional matrix of N measurements, each in\n            format (x, y, a, h) where (x, y) is the bounding box center\n            position, a the aspect ratio, and h the height.\n        only_position : Optional[bool]\n            If True, distance computation is done with respect to the bounding\n            box center position only.\n        Returns\n        -------\n        ndarray\n            Returns an array of length N, where the i-th element contains the\n            squared Mahalanobis distance between (mean, covariance) and\n            `measurements[i]`.\n        \"\"\"\n        mean, covariance = self.project(mean, covariance)\n        if only_position:\n            mean, covariance = mean[:2], covariance[:2, :2]\n            measurements = measurements[:, :2]\n\n        cholesky_factor = np.linalg.cholesky(covariance)\n        d = measurements - mean\n        z = scipy.linalg.solve_triangular(\n            cholesky_factor, d.T, lower=True, check_finite=False,\n            overwrite_b=True)\n        squared_maha = np.sum(z * z, axis=0)\n        return squared_maha","metadata":{"execution":{"iopub.status.busy":"2024-03-22T18:21:28.803335Z","iopub.execute_input":"2024-03-22T18:21:28.803596Z","iopub.status.idle":"2024-03-22T18:21:28.833942Z","shell.execute_reply.started":"2024-03-22T18:21:28.803565Z","shell.execute_reply":"2024-03-22T18:21:28.833129Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"markdown","source":"# IoU Matching\n\nTo associate the detections with a previously tracked object in the frame.","metadata":{}},{"cell_type":"markdown","source":"## Linear Assignment Problem","metadata":{}},{"cell_type":"code","source":"INFTY_COST = 1e+5\n\n\ndef min_cost_matching(\n        distance_metric, max_distance, tracks, detections, track_indices=None,\n        detection_indices=None):\n    \"\"\"Solve linear assignment problem.\n    Parameters\n    ----------\n    distance_metric : Callable[List[Track], List[Detection], List[int], List[int]) -> ndarray\n        The distance metric is given a list of tracks and detections as well as\n        a list of N track indices and M detection indices. The metric should\n        return the NxM dimensional cost matrix, where element (i, j) is the\n        association cost between the i-th track in the given track indices and\n        the j-th detection in the given detection_indices.\n    max_distance : float\n        Gating threshold. Associations with cost larger than this value are\n        disregarded.\n    tracks : List[track.Track]\n        A list of predicted tracks at the current time step.\n    detections : List[detection.Detection]\n        A list of detections at the current time step.\n    track_indices : List[int]\n        List of track indices that maps rows in `cost_matrix` to tracks in\n        `tracks` (see description above).\n    detection_indices : List[int]\n        List of detection indices that maps columns in `cost_matrix` to\n        detections in `detections` (see description above).\n    Returns\n    -------\n    (List[(int, int)], List[int], List[int])\n        Returns a tuple with the following three entries:\n        * A list of matched track and detection indices.\n        * A list of unmatched track indices.\n        * A list of unmatched detection indices.\n    \"\"\"\n    if track_indices is None:\n        track_indices = np.arange(len(tracks))\n    if detection_indices is None:\n        detection_indices = np.arange(len(detections))\n\n    if len(detection_indices) == 0 or len(track_indices) == 0:\n        return [], track_indices, detection_indices  # Nothing to match.\n\n    cost_matrix = distance_metric(\n        tracks, detections, track_indices, detection_indices)\n    cost_matrix[cost_matrix > max_distance] = max_distance + 1e-5\n    indices = linear_sum_assignment(cost_matrix)\n    indices = np.asarray(indices)\n    indices = np.transpose(indices)\n    matches, unmatched_tracks, unmatched_detections = [], [], []\n    for col, detection_idx in enumerate(detection_indices):\n        if col not in indices[:, 1]:\n            unmatched_detections.append(detection_idx)\n    for row, track_idx in enumerate(track_indices):\n        if row not in indices[:, 0]:\n            unmatched_tracks.append(track_idx)\n    for row, col in indices:\n        track_idx = track_indices[row]\n        detection_idx = detection_indices[col]\n        if cost_matrix[row, col] > max_distance:\n            unmatched_tracks.append(track_idx)\n            unmatched_detections.append(detection_idx)\n        else:\n            matches.append((track_idx, detection_idx))\n    return matches, unmatched_tracks, unmatched_detections\n\n\ndef matching_cascade(\n        distance_metric, max_distance, cascade_depth, tracks, detections,\n        track_indices=None, detection_indices=None):\n    \"\"\"Run matching cascade.\n    Parameters\n    ----------\n    distance_metric : Callable[List[Track], List[Detection], List[int], List[int]) -> ndarray\n        The distance metric is given a list of tracks and detections as well as\n        a list of N track indices and M detection indices. The metric should\n        return the NxM dimensional cost matrix, where element (i, j) is the\n        association cost between the i-th track in the given track indices and\n        the j-th detection in the given detection indices.\n    max_distance : float\n        Gating threshold. Associations with cost larger than this value are\n        disregarded.\n    cascade_depth: int\n        The cascade depth, should be se to the maximum track age.\n    tracks : List[track.Track]\n        A list of predicted tracks at the current time step.\n    detections : List[detection.Detection]\n        A list of detections at the current time step.\n    track_indices : Optional[List[int]]\n        List of track indices that maps rows in `cost_matrix` to tracks in\n        `tracks` (see description above). Defaults to all tracks.\n    detection_indices : Optional[List[int]]\n        List of detection indices that maps columns in `cost_matrix` to\n        detections in `detections` (see description above). Defaults to all\n        detections.\n    Returns\n    -------\n    (List[(int, int)], List[int], List[int])\n        Returns a tuple with the following three entries:\n        * A list of matched track and detection indices.\n        * A list of unmatched track indices.\n        * A list of unmatched detection indices.\n    \"\"\"\n    if track_indices is None:\n        track_indices = list(range(len(tracks)))\n    if detection_indices is None:\n        detection_indices = list(range(len(detections)))\n\n    unmatched_detections = detection_indices\n    matches = []\n    for level in range(cascade_depth):\n        if len(unmatched_detections) == 0:  # No detections left\n            break\n\n        track_indices_l = [\n            k for k in track_indices\n            if tracks[k].time_since_update == 1 + level\n        ]\n        if len(track_indices_l) == 0:  # Nothing to match at this level\n            continue\n\n        matches_l, _, unmatched_detections = \\\n            min_cost_matching(\n                distance_metric, max_distance, tracks, detections,\n                track_indices_l, unmatched_detections)\n        matches += matches_l\n    unmatched_tracks = list(set(track_indices) - set(k for k, _ in matches))\n    return matches, unmatched_tracks, unmatched_detections\n\n\ndef gate_cost_matrix(\n        kf, cost_matrix, tracks, detections, track_indices, detection_indices,\n        gated_cost=INFTY_COST, only_position=False):\n    \"\"\"Invalidate infeasible entries in cost matrix based on the state\n    distributions obtained by Kalman filtering.\n    Parameters\n    ----------\n    kf : The Kalman filter.\n    cost_matrix : ndarray\n        The NxM dimensional cost matrix, where N is the number of track indices\n        and M is the number of detection indices, such that entry (i, j) is the\n        association cost between `tracks[track_indices[i]]` and\n        `detections[detection_indices[j]]`.\n    tracks : List[track.Track]\n        A list of predicted tracks at the current time step.\n    detections : List[detection.Detection]\n        A list of detections at the current time step.\n    track_indices : List[int]\n        List of track indices that maps rows in `cost_matrix` to tracks in\n        `tracks` (see description above).\n    detection_indices : List[int]\n        List of detection indices that maps columns in `cost_matrix` to\n        detections in `detections` (see description above).\n    gated_cost : Optional[float]\n        Entries in the cost matrix corresponding to infeasible associations are\n        set this value. Defaults to a very large value.\n    only_position : Optional[bool]\n        If True, only the x, y position of the state distribution is considered\n        during gating. Defaults to False.\n    Returns\n    -------\n    ndarray\n        Returns the modified cost matrix.\n    \"\"\"\n    gating_dim = 2 if only_position else 4\n    gating_threshold = chi2inv95[gating_dim]\n    measurements = np.asarray(\n        [detections[i].to_xyah() for i in detection_indices])\n    for row, track_idx in enumerate(track_indices):\n        track = tracks[track_idx]\n        gating_distance = kf.gating_distance(\n            track.mean, track.covariance, measurements, only_position)\n        cost_matrix[row, gating_distance > gating_threshold] = gated_cost\n    return cost_matrix","metadata":{"execution":{"iopub.status.busy":"2024-03-22T18:21:28.837147Z","iopub.execute_input":"2024-03-22T18:21:28.837459Z","iopub.status.idle":"2024-03-22T18:21:28.862079Z","shell.execute_reply.started":"2024-03-22T18:21:28.837427Z","shell.execute_reply":"2024-03-22T18:21:28.861313Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"markdown","source":"## IoU","metadata":{}},{"cell_type":"code","source":"def iou(bbox, candidates):\n    \"\"\"Computer intersection over union.\n    Parameters\n    ----------\n    bbox : ndarray\n        A bounding box in format `(top left x, top left y, width, height)`.\n    candidates : ndarray\n        A matrix of candidate bounding boxes (one per row) in the same format\n        as `bbox`.\n    Returns\n    -------\n    ndarray\n        The intersection over union in [0, 1] between the `bbox` and each\n        candidate. A higher score means a larger fraction of the `bbox` is\n        occluded by the candidate.\n    \"\"\"\n    bbox_tl, bbox_br = bbox[:2], bbox[:2] + bbox[2:]\n    candidates_tl = candidates[:, :2]\n    candidates_br = candidates[:, :2] + candidates[:, 2:]\n\n    tl = np.c_[np.maximum(bbox_tl[0], candidates_tl[:, 0])[:, np.newaxis],\n               np.maximum(bbox_tl[1], candidates_tl[:, 1])[:, np.newaxis]]\n    br = np.c_[np.minimum(bbox_br[0], candidates_br[:, 0])[:, np.newaxis],\n               np.minimum(bbox_br[1], candidates_br[:, 1])[:, np.newaxis]]\n    wh = np.maximum(0., br - tl)\n\n    area_intersection = wh.prod(axis=1)\n    area_bbox = bbox[2:].prod()\n    area_candidates = candidates[:, 2:].prod(axis=1)\n    return area_intersection / (area_bbox + area_candidates - area_intersection)\n\n\ndef iou_cost(tracks, detections, track_indices=None,\n             detection_indices=None):\n    \"\"\"An intersection over union distance metric.\n    Parameters\n    ----------\n    tracks : List[deep_sort.track.Track]\n        A list of tracks.\n    detections : List[deep_sort.detection.Detection]\n        A list of detections.\n    track_indices : Optional[List[int]]\n        A list of indices to tracks that should be matched. Defaults to\n        all `tracks`.\n    detection_indices : Optional[List[int]]\n        A list of indices to detections that should be matched. Defaults\n        to all `detections`.\n    Returns\n    -------\n    ndarray\n        Returns a cost matrix of shape\n        len(track_indices), len(detection_indices) where entry (i, j) is\n        `1 - iou(tracks[track_indices[i]], detections[detection_indices[j]])`.\n    \"\"\"\n    if track_indices is None:\n        track_indices = np.arange(len(tracks))\n    if detection_indices is None:\n        detection_indices = np.arange(len(detections))\n\n    cost_matrix = np.zeros((len(track_indices), len(detection_indices)))\n    for row, track_idx in enumerate(track_indices):\n        if tracks[track_idx].time_since_update > 1:\n            cost_matrix[row, :] = linear_assignment.INFTY_COST\n            continue\n\n        bbox = tracks[track_idx].to_tlwh()\n        candidates = np.asarray([detections[i].tlwh for i in detection_indices])\n        cost_matrix[row, :] = 1. - iou(bbox, candidates)\n    return cost_matrix","metadata":{"execution":{"iopub.status.busy":"2024-03-22T18:21:28.863450Z","iopub.execute_input":"2024-03-22T18:21:28.864121Z","iopub.status.idle":"2024-03-22T18:21:28.879653Z","shell.execute_reply.started":"2024-03-22T18:21:28.864079Z","shell.execute_reply":"2024-03-22T18:21:28.878967Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"def non_max_suppression(boxes, classes, max_bbox_overlap, scores=None):\n    \"\"\"Suppress overlapping detections.\n    Original code from [1]_ has been adapted to include confidence score.\n    .. [1] http://www.pyimagesearch.com/2015/02/16/\n           faster-non-maximum-suppression-python/\n    Examples\n    --------\n        >>> boxes = [d.roi for d in detections]\n        >>> classes = [d.classes for d in detections]\n        >>> scores = [d.confidence for d in detections]\n        >>> indices = non_max_suppression(boxes, max_bbox_overlap, scores)\n        >>> detections = [detections[i] for i in indices]\n    Parameters\n    ----------\n    boxes : ndarray\n        Array of ROIs (x, y, width, height).\n    max_bbox_overlap : float\n        ROIs that overlap more than this values are suppressed.\n    scores : Optional[array_like]\n        Detector confidence score.\n    Returns\n    -------\n    List[int]\n        Returns indices of detections that have survived non-maxima suppression.\n    \"\"\"\n    if len(boxes) == 0:\n        return []\n\n    boxes = boxes.astype(np.float)\n    pick = []\n\n    x1 = boxes[:, 0]\n    y1 = boxes[:, 1]\n    x2 = boxes[:, 2] + boxes[:, 0]\n    y2 = boxes[:, 3] + boxes[:, 1]\n\n    area = (x2 - x1 + 1) * (y2 - y1 + 1)\n    if scores is not None:\n        idxs = np.argsort(scores)\n    else:\n        idxs = np.argsort(y2)\n\n    while len(idxs) > 0:\n        last = len(idxs) - 1\n        i = idxs[last]\n        pick.append(i)\n\n        xx1 = np.maximum(x1[i], x1[idxs[:last]])\n        yy1 = np.maximum(y1[i], y1[idxs[:last]])\n        xx2 = np.minimum(x2[i], x2[idxs[:last]])\n        yy2 = np.minimum(y2[i], y2[idxs[:last]])\n\n        w = np.maximum(0, xx2 - xx1 + 1)\n        h = np.maximum(0, yy2 - yy1 + 1)\n\n        overlap = (w * h) / area[idxs[:last]]\n\n        idxs = np.delete(\n            idxs, np.concatenate(\n                ([last], np.where(overlap > max_bbox_overlap)[0])))\n\n    return pick","metadata":{"execution":{"iopub.status.busy":"2024-03-22T18:21:28.881021Z","iopub.execute_input":"2024-03-22T18:21:28.881448Z","iopub.status.idle":"2024-03-22T18:21:28.896483Z","shell.execute_reply.started":"2024-03-22T18:21:28.881408Z","shell.execute_reply":"2024-03-22T18:21:28.895613Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"markdown","source":"# Nearest Neighbor Matching\n\nMetric to determine the similarity of two detections.","metadata":{}},{"cell_type":"code","source":"def _pdist(a, b):\n    \"\"\"Compute pair-wise squared distance between points in `a` and `b`.\n    Parameters\n    ----------\n    a : array_like\n        An NxM matrix of N samples of dimensionality M.\n    b : array_like\n        An LxM matrix of L samples of dimensionality M.\n    Returns\n    -------\n    ndarray\n        Returns a matrix of size len(a), len(b) such that eleement (i, j)\n        contains the squared distance between `a[i]` and `b[j]`.\n    \"\"\"\n    a, b = np.asarray(a), np.asarray(b)\n    if len(a) == 0 or len(b) == 0:\n        return np.zeros((len(a), len(b)))\n    a2, b2 = np.square(a).sum(axis=1), np.square(b).sum(axis=1)\n    r2 = -2. * np.dot(a, b.T) + a2[:, None] + b2[None, :]\n    r2 = np.clip(r2, 0., float(np.inf))\n    return r2\n\n\ndef _cosine_distance(a, b, data_is_normalized=False):\n    \"\"\"Compute pair-wise cosine distance between points in `a` and `b`.\n    Parameters\n    ----------\n    a : array_like\n        An NxM matrix of N samples of dimensionality M.\n    b : array_like\n        An LxM matrix of L samples of dimensionality M.\n    data_is_normalized : Optional[bool]\n        If True, assumes rows in a and b are unit length vectors.\n        Otherwise, a and b are explicitly normalized to lenght 1.\n    Returns\n    -------\n    ndarray\n        Returns a matrix of size len(a), len(b) such that eleement (i, j)\n        contains the squared distance between `a[i]` and `b[j]`.\n    \"\"\"\n    if not data_is_normalized:\n        a = np.asarray(a) / np.linalg.norm(a, axis=1, keepdims=True)\n        b = np.asarray(b) / np.linalg.norm(b, axis=1, keepdims=True)\n    return 1. - np.dot(a, b.T)\n\n\ndef _nn_euclidean_distance(x, y):\n    \"\"\" Helper function for nearest neighbor distance metric (Euclidean).\n    Parameters\n    ----------\n    x : ndarray\n        A matrix of N row-vectors (sample points).\n    y : ndarray\n        A matrix of M row-vectors (query points).\n    Returns\n    -------\n    ndarray\n        A vector of length M that contains for each entry in `y` the\n        smallest Euclidean distance to a sample in `x`.\n    \"\"\"\n    distances = _pdist(x, y)\n    return np.maximum(0.0, distances.min(axis=0))\n\n\ndef _nn_cosine_distance(x, y):\n    \"\"\" Helper function for nearest neighbor distance metric (cosine).\n    Parameters\n    ----------\n    x : ndarray\n        A matrix of N row-vectors (sample points).\n    y : ndarray\n        A matrix of M row-vectors (query points).\n    Returns\n    -------\n    ndarray\n        A vector of length M that contains for each entry in `y` the\n        smallest cosine distance to a sample in `x`.\n    \"\"\"\n    distances = _cosine_distance(x, y)\n    return distances.min(axis=0)\n\n\nclass NearestNeighborDistanceMetric(object):\n    \"\"\"\n    A nearest neighbor distance metric that, for each target, returns\n    the closest distance to any sample that has been observed so far.\n    Parameters\n    ----------\n    metric : str\n        Either \"euclidean\" or \"cosine\".\n    matching_threshold: float\n        The matching threshold. Samples with larger distance are considered an\n        invalid match.\n    budget : Optional[int]\n        If not None, fix samples per class to at most this number. Removes\n        the oldest samples when the budget is reached.\n    Attributes\n    ----------\n    samples : Dict[int -> List[ndarray]]\n        A dictionary that maps from target identities to the list of samples\n        that have been observed so far.\n    \"\"\"\n\n    def __init__(self, metric, matching_threshold, budget=None):\n\n\n        if metric == \"euclidean\":\n            self._metric = _nn_euclidean_distance\n        elif metric == \"cosine\":\n            self._metric = _nn_cosine_distance\n        else:\n            raise ValueError(\n                \"Invalid metric; must be either 'euclidean' or 'cosine'\")\n        self.matching_threshold = matching_threshold\n        self.budget = budget\n        self.samples = {}\n\n    def partial_fit(self, features, targets, active_targets):\n        \"\"\"Update the distance metric with new data.\n        Parameters\n        ----------\n        features : ndarray\n            An NxM matrix of N features of dimensionality M.\n        targets : ndarray\n            An integer array of associated target identities.\n        active_targets : List[int]\n            A list of targets that are currently present in the scene.\n        \"\"\"\n        for feature, target in zip(features, targets):\n            self.samples.setdefault(target, []).append(feature)\n            if self.budget is not None:\n                self.samples[target] = self.samples[target][-self.budget:]\n        self.samples = {k: self.samples[k] for k in active_targets}\n\n    def distance(self, features, targets):\n        \"\"\"Compute distance between features and targets.\n        Parameters\n        ----------\n        features : ndarray\n            An NxM matrix of N features of dimensionality M.\n        targets : List[int]\n            A list of targets to match the given `features` against.\n        Returns\n        -------\n        ndarray\n            Returns a cost matrix of shape len(targets), len(features), where\n            element (i, j) contains the closest squared distance between\n            `targets[i]` and `features[j]`.\n        \"\"\"\n        cost_matrix = np.zeros((len(targets), len(features)))\n        for i, target in enumerate(targets):\n            cost_matrix[i, :] = self._metric(self.samples[target], features)\n        return cost_matrix","metadata":{"execution":{"iopub.status.busy":"2024-03-22T18:21:28.897828Z","iopub.execute_input":"2024-03-22T18:21:28.898067Z","iopub.status.idle":"2024-03-22T18:21:28.919564Z","shell.execute_reply.started":"2024-03-22T18:21:28.898032Z","shell.execute_reply":"2024-03-22T18:21:28.918738Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"markdown","source":"# Tracking\n\nTracking an object in a series of frames","metadata":{}},{"cell_type":"code","source":"class TrackState:\n    \"\"\"\n    Enumeration type for the single target track state. Newly created tracks are\n    classified as `tentative` until enough evidence has been collected. Then,\n    the track state is changed to `confirmed`. Tracks that are no longer alive\n    are classified as `deleted` to mark them for removal from the set of active\n    tracks.\n    \"\"\"\n\n    Tentative = 1\n    Confirmed = 2\n    Deleted = 3\n\n\nclass Track:\n    \"\"\"\n    A single target track with state space `(x, y, a, h)` and associated\n    velocities, where `(x, y)` is the center of the bounding box, `a` is the\n    aspect ratio and `h` is the height.\n    Parameters\n    ----------\n    mean : ndarray\n        Mean vector of the initial state distribution.\n    covariance : ndarray\n        Covariance matrix of the initial state distribution.\n    track_id : int\n        A unique track identifier.\n    n_init : int\n        Number of consecutive detections before the track is confirmed. The\n        track state is set to `Deleted` if a miss occurs within the first\n        `n_init` frames.\n    max_age : int\n        The maximum number of consecutive misses before the track state is\n        set to `Deleted`.\n    feature : Optional[ndarray]\n        Feature vector of the detection this track originates from. If not None,\n        this feature is added to the `features` cache.\n    Attributes\n    ----------\n    mean : ndarray\n        Mean vector of the initial state distribution.\n    covariance : ndarray\n        Covariance matrix of the initial state distribution.\n    track_id : int\n        A unique track identifier.\n    hits : int\n        Total number of measurement updates.\n    age : int\n        Total number of frames since first occurance.\n    time_since_update : int\n        Total number of frames since last measurement update.\n    state : TrackState\n        The current track state.\n    features : List[ndarray]\n        A cache of features. On each measurement update, the associated feature\n        vector is added to this list.\n    \"\"\"\n\n    def __init__(self, mean, covariance, track_id, n_init, max_age,\n                 feature=None, class_name=None):\n        self.mean = mean\n        self.covariance = covariance\n        self.track_id = track_id\n        self.hits = 1\n        self.age = 1\n        self.time_since_update = 0\n\n        self.state = TrackState.Tentative\n        self.features = []\n        if feature is not None:\n            self.features.append(feature)\n\n        self._n_init = n_init\n        self._max_age = max_age\n        self.class_name = class_name\n\n    def to_tlwh(self):\n        \"\"\"Get current position in bounding box format `(top left x, top left y,\n        width, height)`.\n        Returns\n        -------\n        ndarray\n            The bounding box.\n        \"\"\"\n        ret = self.mean[:4].copy()\n        ret[2] *= ret[3]\n        ret[:2] -= ret[2:] / 2\n        return ret\n\n    def to_tlbr(self):\n        \"\"\"Get current position in bounding box format `(min x, miny, max x,\n        max y)`.\n        Returns\n        -------\n        ndarray\n            The bounding box.\n        \"\"\"\n        ret = self.to_tlwh()\n        ret[2:] = ret[:2] + ret[2:]\n        return ret\n    \n    def get_class(self):\n        return self.class_name\n\n    def predict(self, kf):\n        \"\"\"Propagate the state distribution to the current time step using a\n        Kalman filter prediction step.\n        Parameters\n        ----------\n        kf : kalman_filter.KalmanFilter\n            The Kalman filter.\n        \"\"\"\n        self.mean, self.covariance = kf.predict(self.mean, self.covariance)\n        self.age += 1\n        self.time_since_update += 1\n\n    def update(self, kf, detection):\n        \"\"\"Perform Kalman filter measurement update step and update the feature\n        cache.\n        Parameters\n        ----------\n        kf : kalman_filter.KalmanFilter\n            The Kalman filter.\n        detection : Detection\n            The associated detection.\n        \"\"\"\n        self.mean, self.covariance = kf.update(\n            self.mean, self.covariance, detection.to_xyah())\n        self.features.append(detection.feature)\n\n        self.hits += 1\n        self.time_since_update = 0\n        if self.state == TrackState.Tentative and self.hits >= self._n_init:\n            self.state = TrackState.Confirmed\n\n    def mark_missed(self):\n        \"\"\"Mark this track as missed (no association at the current time step).\n        \"\"\"\n        if self.state == TrackState.Tentative:\n            self.state = TrackState.Deleted\n        elif self.time_since_update > self._max_age:\n            self.state = TrackState.Deleted\n\n    def is_tentative(self):\n        \"\"\"Returns True if this track is tentative (unconfirmed).\n        \"\"\"\n        return self.state == TrackState.Tentative\n\n    def is_confirmed(self):\n        \"\"\"Returns True if this track is confirmed.\"\"\"\n        return self.state == TrackState.Confirmed\n\n    def is_deleted(self):\n        \"\"\"Returns True if this track is dead and should be deleted.\"\"\"\n        return self.state == TrackState.Deleted","metadata":{"execution":{"iopub.status.busy":"2024-03-22T18:21:28.920954Z","iopub.execute_input":"2024-03-22T18:21:28.921201Z","iopub.status.idle":"2024-03-22T18:21:28.940809Z","shell.execute_reply.started":"2024-03-22T18:21:28.921172Z","shell.execute_reply":"2024-03-22T18:21:28.940040Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"class Tracker:\n    \"\"\"\n    This is the multi-target tracker.\n    Parameters\n    ----------\n    metric : nn_matching.NearestNeighborDistanceMetric\n        A distance metric for measurement-to-track association.\n    max_age : int\n        Maximum number of missed misses before a track is deleted.\n    n_init : int\n        Number of consecutive detections before the track is confirmed. The\n        track state is set to `Deleted` if a miss occurs within the first\n        `n_init` frames.\n    Attributes\n    ----------\n    metric : nn_matching.NearestNeighborDistanceMetric\n        The distance metric used for measurement to track association.\n    max_age : int\n        Maximum number of missed misses before a track is deleted.\n    n_init : int\n        Number of frames that a track remains in initialization phase.\n    kf : kalman_filter.KalmanFilter\n        A Kalman filter to filter target trajectories in image space.\n    tracks : List[Track]\n        The list of active tracks at the current time step.\n    \"\"\"\n\n    def __init__(self, metric, max_iou_distance=0.7, max_age=30, n_init=3):\n        self.metric = metric\n        self.max_iou_distance = max_iou_distance\n        self.max_age = max_age\n        self.n_init = n_init\n\n        self.kf = KalmanFilter()\n        self.tracks = []\n        self._next_id = 1\n\n    def predict(self):\n        \"\"\"Propagate track state distributions one time step forward.\n        This function should be called once every time step, before `update`.\n        \"\"\"\n        for track in self.tracks:\n            track.predict(self.kf)\n\n    def update(self, detections):\n        \"\"\"Perform measurement update and track management.\n        Parameters\n        ----------\n        detections : List[deep_sort.detection.Detection]\n            A list of detections at the current time step.\n        \"\"\"\n        # Run matching cascade.\n        matches, unmatched_tracks, unmatched_detections = \\\n            self._match(detections)\n\n        # Update track set.\n        for track_idx, detection_idx in matches:\n            self.tracks[track_idx].update(\n                self.kf, detections[detection_idx])\n        for track_idx in unmatched_tracks:\n            self.tracks[track_idx].mark_missed()\n        for detection_idx in unmatched_detections:\n            self._initiate_track(detections[detection_idx])\n        self.tracks = [t for t in self.tracks if not t.is_deleted()]\n\n        # Update distance metric.\n        active_targets = [t.track_id for t in self.tracks if t.is_confirmed()]\n        features, targets = [], []\n        for track in self.tracks:\n            if not track.is_confirmed():\n                continue\n            features += track.features\n            targets += [track.track_id for _ in track.features]\n            track.features = []\n        self.metric.partial_fit(\n            np.asarray(features), np.asarray(targets), active_targets)\n\n    def _match(self, detections):\n\n        def gated_metric(tracks, dets, track_indices, detection_indices):\n            features = np.array([dets[i].feature for i in detection_indices])\n            targets = np.array([tracks[i].track_id for i in track_indices])\n            cost_matrix = self.metric.distance(features, targets)\n            cost_matrix = gate_cost_matrix(\n                self.kf, cost_matrix, tracks, dets, track_indices,\n                detection_indices)\n\n            return cost_matrix\n\n        # Split track set into confirmed and unconfirmed tracks.\n        confirmed_tracks = [\n            i for i, t in enumerate(self.tracks) if t.is_confirmed()]\n        unconfirmed_tracks = [\n            i for i, t in enumerate(self.tracks) if not t.is_confirmed()]\n\n        # Associate confirmed tracks using appearance features.\n        matches_a, unmatched_tracks_a, unmatched_detections = \\\n                matching_cascade(\n                gated_metric, self.metric.matching_threshold, self.max_age,\n                self.tracks, detections, confirmed_tracks)\n\n        # Associate remaining tracks together with unconfirmed tracks using IOU.\n        iou_track_candidates = unconfirmed_tracks + [\n            k for k in unmatched_tracks_a if\n            self.tracks[k].time_since_update == 1]\n        unmatched_tracks_a = [\n            k for k in unmatched_tracks_a if\n            self.tracks[k].time_since_update != 1]\n        matches_b, unmatched_tracks_b, unmatched_detections = \\\n                min_cost_matching(\n                iou_cost, self.max_iou_distance, self.tracks,\n                detections, iou_track_candidates, unmatched_detections)\n\n        matches = matches_a + matches_b\n        unmatched_tracks = list(set(unmatched_tracks_a + unmatched_tracks_b))\n        return matches, unmatched_tracks, unmatched_detections\n\n    def _initiate_track(self, detection):\n        mean, covariance = self.kf.initiate(detection.to_xyah())\n        class_name = detection.get_class()\n        self.tracks.append(Track(\n            mean, covariance, self._next_id, self.n_init, self.max_age,\n            detection.feature, class_name))\n        self._next_id += 1","metadata":{"execution":{"iopub.status.busy":"2024-03-22T18:21:28.942039Z","iopub.execute_input":"2024-03-22T18:21:28.942265Z","iopub.status.idle":"2024-03-22T18:21:28.965476Z","shell.execute_reply.started":"2024-03-22T18:21:28.942237Z","shell.execute_reply":"2024-03-22T18:21:28.964669Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"markdown","source":"# Object Tracking\n\nCombining everything together","metadata":{}},{"cell_type":"code","source":"# Definition of the parameters\nmax_cosine_distance = 0.7\nmax_euclidean_distance = 0.7\nnn_budget = None\nmodel_filename = 'mars-small128.pb'\n\nkey_list = list(NUM_CLASS.keys()) \nval_list = list(NUM_CLASS.values()) ","metadata":{"execution":{"iopub.status.busy":"2024-03-22T18:21:28.966703Z","iopub.execute_input":"2024-03-22T18:21:28.967425Z","iopub.status.idle":"2024-03-22T18:21:28.979663Z","shell.execute_reply.started":"2024-03-22T18:21:28.967383Z","shell.execute_reply":"2024-03-22T18:21:28.978934Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"import time\n\n# Function for object tracking on video\ndef object_tracking(video_path, output_path):\n    encoder = create_box_encoder(model_filename, batch_size=1)\n    metric = NearestNeighborDistanceMetric(\"cosine\", max_cosine_distance, nn_budget)\n    tracker = Tracker(metric)\n    \n    times, times_2 = [], []\n    vid = cv2.VideoCapture(video_path) # detect on video\n\n    # by default VideoCapture returns float instead of int\n    width = int(vid.get(cv2.CAP_PROP_FRAME_WIDTH))\n    height = int(vid.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    fps = int(vid.get(cv2.CAP_PROP_FPS))\n#     codec = cv2.VideoWriter_fourcc(*'XVID')\n#     codec = cv2.VideoWriter_fourcc(*'H264') \n#     codec = cv2.VideoWriter_fourcc(*'MJGP') \n### to solve this issue: \n## \"OpenCV: FFMPEG: tag 0x44495658/'XVID' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n## OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\"\n    codec = cv2.VideoWriter_fourcc('m', 'p', '4', 'v') \n\n    out = cv2.VideoWriter(output_path, codec, fps, (width, height)) # output_path must be .mp4\n    \n    while True:\n        _, frame = vid.read()\n\n        try:\n            original_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n            original_frame = cv2.cvtColor(original_frame, cv2.COLOR_BGR2RGB)\n        except:\n            break\n\n        # Load picture with old dimensions\n        image_data, image_w, image_h = image_preprocess(np.copy(original_frame), (WIDTH, HEIGHT))\n\n        # Predict Image\n        t1 = time.time()\n        yhat = model.predict(image_data)\n\n        # Create boxes\n        boxes = list()\n        for i in range(len(yhat)):\n            boxes += decode_netout(yhat[i][0], anchors[i], class_threshold, HEIGHT, WIDTH)\n\n        # Correct the sizes of boxes for shape of image\n        correct_yolo_boxes(boxes, image_h, image_w, HEIGHT, WIDTH)\n\n        # Suppress Non Maximal Boxes\n        do_nms(boxes, 0.3)\n\n        # define the labels (Filtered only the ones relevant for this task, which were used in pretraining the YOLOv3 model)\n        labels = [\"person\", \"bicycle\", \"car\", \"motorbike\", \"aeroplane\", \"bus\", \"train\", \"truck\",\"boat\"]\n\n        # get the details of the detected objects\n        v_boxes, v_labels, v_scores = get_boxes(boxes, labels, class_threshold)\n\n\n        #t1 = time.time()\n        #pred_bbox = Yolo.predict(image_data)\n        t2 = time.time()\n\n        # Obtain all the detections for the given frame\n        boxes = []\n        names = []\n        scores = []\n        for box, label, score in zip(v_boxes, v_labels, v_scores):\n            boxes.append([\n                 box.xmin, box.ymin, \n                (box.xmax - box.xmin), (box.ymax - box.ymin) * 2])\n            names.append(label)\n            scores.append(score)\n\n        boxes = np.array(boxes) \n        names = np.array(names)\n        scores = np.array(scores)\n        features = np.array(encoder(original_frame, boxes))\n        detections = [Detection(bbox, score, class_name, feature) for bbox, score, class_name, feature in zip(boxes, scores, names, features)]\n\n        # Pass detections to the deepsort object and obtain the track information.\n        tracker.predict()\n        tracker.update(detections)\n\n        # Obtain info from the tracks\n        tracked_bboxes = []\n        for track in tracker.tracks:\n            if not track.is_confirmed() or track.time_since_update > 5:\n                continue \n            bbox = track.to_tlbr() # Get the corrected/predicted bounding box\n            class_name = track.get_class() #Get the class name of particular object\n            tracking_id = track.track_id # Get the ID for the particular track\n            index = key_list[val_list.index(class_name)] # Get predicted object index by object name\n            tracked_bboxes.append(bbox.tolist() + [tracking_id, index]) # Structure data, that we could use it with our draw_bbox function\n\n        # draw detection on frame\n        image = draw_bbox(original_frame, tracked_bboxes, CLASSES=NUM_CLASS, tracking=True)\n\n        t3 = time.time()\n        times.append(t2-t1)\n        times_2.append(t3-t1)\n\n        times = times[-20:]\n        times_2 = times_2[-20:]\n\n        ms = sum(times)/len(times)*1000\n        fps = 1000 / ms\n        fps2 = 1000 / (sum(times_2)/len(times_2)*1000)\n\n        image = cv2.putText(image, \"Time: {:.1f} FPS\".format(fps), (0, 30), cv2.FONT_HERSHEY_COMPLEX_SMALL, 1, (0, 0, 255), 2)\n\n        # draw original yolo detection\n        #image = draw_bbox(image, bboxes, CLASSES=CLASSES, show_label=False, rectangle_colors=rectangle_colors, tracking=True)\n\n        print(\"Time: {:.2f}ms, Detection FPS: {:.1f}, total FPS: {:.1f}\".format(ms, fps, fps2))\n        out.write(image)\n\n    out.release()\n    vid.release()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Process all Videos\nimport os\n\nVIDEO_PATH = '../input/kittiroadsegmentation/testing/'\nvideos = os.listdir(VIDEO_PATH)\n\nfor video in videos:\n    print(f\"Processing video {video}\")\n    object_tracking(VIDEO_PATH + video, video)","metadata":{"execution":{"iopub.status.busy":"2024-03-22T18:27:14.315514Z","iopub.execute_input":"2024-03-22T18:27:14.316120Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Processing video challenge.mp4\nTime: 4546.59ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4560.57ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4535.24ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4533.53ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4538.76ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4561.99ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4551.84ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4547.61ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4556.13ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4548.00ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4542.88ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4564.75ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4589.36ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4607.52ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4624.90ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4639.13ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4654.15ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4662.92ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4679.87ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4686.48ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4702.46ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4713.31ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4731.09ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4744.22ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4760.69ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4786.65ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4799.39ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4812.73ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4826.89ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4843.24ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4858.43ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4865.36ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4861.85ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4852.55ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4848.87ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4839.25ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4830.49ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4827.13ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4822.87ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4814.56ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4804.34ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4799.28ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4790.02ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4785.83ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4789.72ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4768.26ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4771.41ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4772.29ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4768.14ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4772.64ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4779.28ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4779.21ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4790.88ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4800.59ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4800.57ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4808.02ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4817.56ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4826.61ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4826.03ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4835.54ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4847.24ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4858.59ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4871.58ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4888.32ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4884.63ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4905.12ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4899.75ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4901.51ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4901.29ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4905.45ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4901.77ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4895.18ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4880.34ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4885.83ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4889.90ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4899.77ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4902.87ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4905.42ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4919.96ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4928.67ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4935.76ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4942.58ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4950.68ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4950.14ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4954.21ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4946.77ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4960.76ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4970.21ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4979.54ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4981.48ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4985.04ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4984.76ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4998.94ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4991.34ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4991.56ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4989.11ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4979.30ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4973.45ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4950.19ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4932.25ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4922.87ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4911.81ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4890.05ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4873.16ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4857.45ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4851.34ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4842.33ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4829.96ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4814.78ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4797.74ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4784.89ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4781.90ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4771.04ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4776.47ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4778.43ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4773.38ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4778.39ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4773.74ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4777.38ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4801.87ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4804.46ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4802.70ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4810.65ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4812.30ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4819.87ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4820.29ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4824.49ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4828.56ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4832.09ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4834.65ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4842.21ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4849.54ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4867.77ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4863.72ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4861.27ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4858.50ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4855.80ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4852.86ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4861.99ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4841.32ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4839.32ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4840.48ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4835.87ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4838.21ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4841.64ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4857.84ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4862.86ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4872.59ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4884.73ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4900.78ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4919.84ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4925.79ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4922.21ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4928.04ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4935.02ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4945.92ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4961.70ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4981.80ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4984.00ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 5005.57ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 5013.33ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 5021.26ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 5040.92ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 5050.61ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 5058.63ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 5039.53ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 5035.74ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 5028.92ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 5026.20ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 5022.55ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 5009.11ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 5008.70ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 5013.03ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 5012.45ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 5011.42ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 5010.96ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 5002.96ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4998.19ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4997.09ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4989.37ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4988.14ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4992.17ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4986.82ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4989.34ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4988.42ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 4990.90ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 5014.60ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 5022.84ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 5026.40ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 5029.02ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 5043.93ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 5043.11ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 5039.68ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 5045.88ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 5046.47ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 5048.82ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 5055.01ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 5055.29ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 5056.86ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 5072.14ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 5070.80ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 5068.41ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 5068.92ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 5066.77ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 5064.15ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 5070.98ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 5050.16ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 5043.24ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 5044.83ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 5042.62ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 5032.42ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 5036.07ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 5035.79ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 5043.88ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 5047.42ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 5046.00ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 5043.79ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 5048.62ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 5046.42ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 5031.17ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 5034.91ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 5032.61ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 5029.63ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 5034.92ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 5037.27ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 5034.05ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 5048.88ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 5052.55ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 5048.30ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 5052.63ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 5051.53ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 5046.93ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 5041.68ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 5028.59ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 5022.45ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 5019.37ms, Detection FPS: 0.2, total FPS: 0.2\nTime: 5017.14ms, Detection FPS: 0.2, total FPS: 0.2\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}